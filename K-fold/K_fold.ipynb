{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a74b1e63",
   "metadata": {},
   "source": [
    "## Assignment points :\n",
    "    1. Load MINST dataset\n",
    "    2. Subset your data to use only class 0 and class1 for the next steps.\n",
    "    3. Standardize your dataset\n",
    "    4. Divide data into training and validation set using 10-fold cross validation method\n",
    "    5. Implement Logistic Regression with different values for learning rate\n",
    "    6. Report difference accuracy for the different learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d84d263",
   "metadata": {},
   "source": [
    "## 1. Load MINST dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da700e3e",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9778a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42d6f910",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f21703",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_digits()\n",
    "df = pd.DataFrame(mnist.data)\n",
    "df['Target'] = pd.DataFrame(mnist.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cb2b7a4",
   "metadata": {},
   "source": [
    "### Data explore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02f8c715",
   "metadata": {},
   "source": [
    "#### Frame explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24638839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1     2     3     4     5    6    7    8    9  ...   55   56   \n",
       "0     0.0  0.0   5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  \\\n",
       "1     0.0  0.0   0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0   0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3     0.0  0.0   7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  0.0  0.0   \n",
       "4     0.0  0.0   0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...   ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1792  0.0  0.0   4.0  10.0  13.0   6.0  0.0  0.0  0.0  1.0  ...  0.0  0.0   \n",
       "1793  0.0  0.0   6.0  16.0  13.0  11.0  1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1794  0.0  0.0   1.0  11.0  15.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1795  0.0  0.0   2.0  10.0   7.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1796  0.0  0.0  10.0  14.0   8.0   1.0  0.0  0.0  0.0  2.0  ...  0.0  0.0   \n",
       "\n",
       "       57   58    59    60    61   62   63  Target  \n",
       "0     0.0  6.0  13.0  10.0   0.0  0.0  0.0       0  \n",
       "1     0.0  0.0  11.0  16.0  10.0  0.0  0.0       1  \n",
       "2     0.0  0.0   3.0  11.0  16.0  9.0  0.0       2  \n",
       "3     0.0  7.0  13.0  13.0   9.0  0.0  0.0       3  \n",
       "4     0.0  0.0   2.0  16.0   4.0  0.0  0.0       4  \n",
       "...   ...  ...   ...   ...   ...  ...  ...     ...  \n",
       "1792  0.0  2.0  14.0  15.0   9.0  0.0  0.0       9  \n",
       "1793  0.0  6.0  16.0  14.0   6.0  0.0  0.0       0  \n",
       "1794  0.0  2.0   9.0  13.0   6.0  0.0  0.0       8  \n",
       "1795  0.0  5.0  12.0  16.0  12.0  0.0  0.0       9  \n",
       "1796  1.0  8.0  12.0  14.0  12.0  1.0  0.0       8  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b65a22d4",
   "metadata": {},
   "source": [
    "#### Visualize digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7da3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 10, figsize=(16, 6))\n",
    "for i in range(20):\n",
    "    axes[i//10, i %10].imshow(mnist.images[i], cmap='gray');\n",
    "    axes[i//10, i %10].axis('off')\n",
    "    axes[i//10, i %10].set_title(f\"target: {mnist.target[i]}\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ed669f5",
   "metadata": {},
   "source": [
    "## 2. Subset your data to use only class 0 and class1 for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36826e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 0 to 1793\n",
      "Data columns (total 65 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       360 non-null    float64\n",
      " 1   1       360 non-null    float64\n",
      " 2   2       360 non-null    float64\n",
      " 3   3       360 non-null    float64\n",
      " 4   4       360 non-null    float64\n",
      " 5   5       360 non-null    float64\n",
      " 6   6       360 non-null    float64\n",
      " 7   7       360 non-null    float64\n",
      " 8   8       360 non-null    float64\n",
      " 9   9       360 non-null    float64\n",
      " 10  10      360 non-null    float64\n",
      " 11  11      360 non-null    float64\n",
      " 12  12      360 non-null    float64\n",
      " 13  13      360 non-null    float64\n",
      " 14  14      360 non-null    float64\n",
      " 15  15      360 non-null    float64\n",
      " 16  16      360 non-null    float64\n",
      " 17  17      360 non-null    float64\n",
      " 18  18      360 non-null    float64\n",
      " 19  19      360 non-null    float64\n",
      " 20  20      360 non-null    float64\n",
      " 21  21      360 non-null    float64\n",
      " 22  22      360 non-null    float64\n",
      " 23  23      360 non-null    float64\n",
      " 24  24      360 non-null    float64\n",
      " 25  25      360 non-null    float64\n",
      " 26  26      360 non-null    float64\n",
      " 27  27      360 non-null    float64\n",
      " 28  28      360 non-null    float64\n",
      " 29  29      360 non-null    float64\n",
      " 30  30      360 non-null    float64\n",
      " 31  31      360 non-null    float64\n",
      " 32  32      360 non-null    float64\n",
      " 33  33      360 non-null    float64\n",
      " 34  34      360 non-null    float64\n",
      " 35  35      360 non-null    float64\n",
      " 36  36      360 non-null    float64\n",
      " 37  37      360 non-null    float64\n",
      " 38  38      360 non-null    float64\n",
      " 39  39      360 non-null    float64\n",
      " 40  40      360 non-null    float64\n",
      " 41  41      360 non-null    float64\n",
      " 42  42      360 non-null    float64\n",
      " 43  43      360 non-null    float64\n",
      " 44  44      360 non-null    float64\n",
      " 45  45      360 non-null    float64\n",
      " 46  46      360 non-null    float64\n",
      " 47  47      360 non-null    float64\n",
      " 48  48      360 non-null    float64\n",
      " 49  49      360 non-null    float64\n",
      " 50  50      360 non-null    float64\n",
      " 51  51      360 non-null    float64\n",
      " 52  52      360 non-null    float64\n",
      " 53  53      360 non-null    float64\n",
      " 54  54      360 non-null    float64\n",
      " 55  55      360 non-null    float64\n",
      " 56  56      360 non-null    float64\n",
      " 57  57      360 non-null    float64\n",
      " 58  58      360 non-null    float64\n",
      " 59  59      360 non-null    float64\n",
      " 60  60      360 non-null    float64\n",
      " 61  61      360 non-null    float64\n",
      " 62  62      360 non-null    float64\n",
      " 63  63      360 non-null    float64\n",
      " 64  Target  360 non-null    int32  \n",
      "dtypes: float64(64), int32(1)\n",
      "memory usage: 184.2 KB\n"
     ]
    }
   ],
   "source": [
    "df_0_1 = df[(df['Target']==0) | (df['Target']==1)]\n",
    "df_0_1.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fcaad71",
   "metadata": {},
   "source": [
    "## 3. Standardize your dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efb90117",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc805e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6927bca9",
   "metadata": {},
   "source": [
    "### Standradize function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a43850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standradize(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    standardized_data = (data - mean) / std\n",
    "    return standardized_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4da09e1",
   "metadata": {},
   "source": [
    "## 4. Divide data into training and validation set using 10-fold cross validation method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0583068a",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eca23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9ca0593",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d2081f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shuffle(df_0_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0076b19",
   "metadata": {},
   "source": [
    "#### K-Fold cross validation implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d06ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "549be525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    true=0\n",
    "    for i in range(len(y)) :\n",
    "        if y[i] == y_hat[i]:\n",
    "            true+=1\n",
    "    return (true / len(y)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa142de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold_cross_validation(X, y, k, eta, epochs):\n",
    "    n = len(X)\n",
    "    \n",
    "    fold_size = n // k\n",
    "    loss = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start, end = i * fold_size, (i + 1) * fold_size\n",
    "        \n",
    "        X_test, y_test = X[start:end], y[start:end]\n",
    "        X_train = np.concatenate((X[:start], X[end:]), axis=0)\n",
    "        y_train = np.concatenate((y[:start], y[end:]), axis=0)\n",
    "        clf = Logistic(eta, epochs)\n",
    "        clf.fit(X_train, y_train, eta, epochs)\n",
    "        \n",
    "        y_hat = clf.predict(X_test)\n",
    "        m = len(y_hat)\n",
    "        w, b = clf.get_weights()\n",
    "        y_pred_proba = sigmoid(X_test @ w.T + b)\n",
    "        log_loss = -np.mean(y_test * np.log(y_pred_proba) + (1 - y_test) * np.log(1 - y_pred_proba))\n",
    "        loss.append(log_loss)\n",
    "        print(\"Loss of Group : \", i+1, \"is : \",  log_loss, \"With eta : \", eta)\n",
    "        print(\"Accuracy of Group : \", i+1, \"is : \", accuracy(y_test, y_hat.astype(int)))\n",
    "        print(\"--------------------------------------------------------------\")\n",
    "       \n",
    "    return loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bfe3aa9",
   "metadata": {},
   "source": [
    "## 5. Implement Logistic Regression with different values for learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc3b311c",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0d6fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f72eb451",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac74217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic:\n",
    "    def __init__(self, eta=0.0008, epochs=100):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.eta = None\n",
    "        self.epochs = None\n",
    "\n",
    "    def unit_step(self, z):\n",
    "        for i in range(len(z)):\n",
    "            if z[i] >0.5:\n",
    "                z[i] = 1\n",
    "            else:\n",
    "                z[i]=0\n",
    "        return z\n",
    "    \n",
    "    def cost(self,y_hat,y):\n",
    "        m = len(y)\n",
    "        return -np.mean(-y * np.log(y_hat) - ((1-y) * np.log(1-y_hat)))\n",
    "    \n",
    "    def get_net_input(self, X, y):\n",
    "        return (np.dot(X, self.w)) + self.b\n",
    "    \n",
    "    def update_weight(self, y_pred, y, X):\n",
    "        return self.w - ((self.eta * np.dot((y_pred - y),X)) / len(X))\n",
    "    \n",
    "    def update_bias(self, y_pred, y):\n",
    "        return self.b - (self.eta * np.mean((y_pred - y)))\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y, eta, epochs):\n",
    "        m = len(X)\n",
    "        self.w = np.random.uniform(low=-0.01, high=0.01, size=X.shape[1])\n",
    "        self.b = np.random.uniform(low=-0.01, high=0.01, size=1)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            #Get net input\n",
    "            z = self.get_net_input(X, y)\n",
    "            \n",
    "            #apply activation function\n",
    "            z = self.activation(z)\n",
    "            \n",
    "            #Calc error\n",
    "            y_pred = z.copy()\n",
    "            error = self.cost(y_pred, y)\n",
    "            #Update Weight and bias\n",
    "            self.w = self.update_weight(y_pred, y, X)\n",
    "            self.b = self.update_bias(y_pred, y)\n",
    "            \n",
    "            if error == 0:\n",
    "                break\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.unit_step((np.dot(self.w, X.T)) + self.b)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [self.w, self.b]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79c3e4f1",
   "metadata": {},
   "source": [
    "## 6. Report difference accuracy for the different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73dfa9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns = ['Target'])\n",
    "y = dataset['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5121ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b074ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.00044565707885151534 With eta :  0.01\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.004256960592164251 With eta :  0.01\n",
      "Accuracy of Group :  2 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  7.511877437764607e-05 With eta :  0.01\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.0009519922692126432 With eta :  0.01\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.00027222051852917304 With eta :  0.01\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.0002924756248340489 With eta :  0.01\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.0006962175569489705 With eta :  0.01\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.0022466452768063884 With eta :  0.01\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.00858259725587137 With eta :  0.01\n",
      "Accuracy of Group :  9 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.00012382205157525507 With eta :  0.01\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.01, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19467a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.0044730406881254145 With eta :  0.001\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.016283733077124423 With eta :  0.001\n",
      "Accuracy of Group :  2 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  0.0013579377407709299 With eta :  0.001\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.00546523281297337 With eta :  0.001\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.003163811738111758 With eta :  0.001\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.0038859759202240756 With eta :  0.001\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.005351118454871361 With eta :  0.001\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.010456877756678445 With eta :  0.001\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.025154287410297498 With eta :  0.001\n",
      "Accuracy of Group :  9 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.002246676827167882 With eta :  0.001\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a962d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.03754240122021958 With eta :  0.0001\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.05985419332722756 With eta :  0.0001\n",
      "Accuracy of Group :  2 is :  97.22222222222221\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  0.02118901290362748 With eta :  0.0001\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.038349060426393834 With eta :  0.0001\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.030877378866537315 With eta :  0.0001\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.033826380763331214 With eta :  0.0001\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.03563908743972187 With eta :  0.0001\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.04656431622412734 With eta :  0.0001\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.08361064238813953 With eta :  0.0001\n",
      "Accuracy of Group :  9 is :  94.44444444444444\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.02521058867371187 With eta :  0.0001\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.0001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a687e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.002430379016910697 With eta :  0.002\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.010386354775022592 With eta :  0.002\n",
      "Accuracy of Group :  2 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  0.0005592447431854152 With eta :  0.002\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.003160005545298341 With eta :  0.002\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.0015912772160447677 With eta :  0.002\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.0018833143897669226 With eta :  0.002\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.0028953038759238943 With eta :  0.002\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.006858178984795862 With eta :  0.002\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.018356686488581204 With eta :  0.002\n",
      "Accuracy of Group :  9 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.001010199928937504 With eta :  0.002\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.002, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba19f4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.005348466512982595 With eta :  0.0008\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.017848100517187934 With eta :  0.0008\n",
      "Accuracy of Group :  2 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  0.0017571088101966327 With eta :  0.0008\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.006794077754227186 With eta :  0.0008\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.0040323114661402135 With eta :  0.0008\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.004980793906444906 With eta :  0.0008\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.006461812787789478 With eta :  0.0008\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.01165229981510062 With eta :  0.0008\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.03147277226960262 With eta :  0.0008\n",
      "Accuracy of Group :  9 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.0028819420790263 With eta :  0.0008\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.0008, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0729f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.02015855632133111 With eta :  0.0002\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.03975564154283552 With eta :  0.0002\n",
      "Accuracy of Group :  2 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  0.009680686719617347 With eta :  0.0002\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.02058477770854485 With eta :  0.0002\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.01538874011341559 With eta :  0.0002\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.018943914046114438 With eta :  0.0002\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.019155829715877145 With eta :  0.0002\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.029062857348374223 With eta :  0.0002\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.05861717383178385 With eta :  0.0002\n",
      "Accuracy of Group :  9 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.012558865692363065 With eta :  0.0002\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.0002, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9137e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Group :  1 is :  0.0015115052247818039 With eta :  0.003\n",
      "Accuracy of Group :  1 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  2 is :  0.00802887002790505 With eta :  0.003\n",
      "Accuracy of Group :  2 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  3 is :  0.0003361771514123013 With eta :  0.003\n",
      "Accuracy of Group :  3 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  4 is :  0.002404155577990575 With eta :  0.003\n",
      "Accuracy of Group :  4 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  5 is :  0.0010105655413018542 With eta :  0.003\n",
      "Accuracy of Group :  5 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  6 is :  0.0012826285522757712 With eta :  0.003\n",
      "Accuracy of Group :  6 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  7 is :  0.0020705267268869153 With eta :  0.003\n",
      "Accuracy of Group :  7 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  8 is :  0.00473180327610307 With eta :  0.003\n",
      "Accuracy of Group :  8 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  9 is :  0.016018749655591472 With eta :  0.003\n",
      "Accuracy of Group :  9 is :  100.0\n",
      "--------------------------------------------------------------\n",
      "Loss of Group :  10 is :  0.0006153986374953761 With eta :  0.003\n",
      "Accuracy of Group :  10 is :  100.0\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss =kFold_cross_validation(X, y, 10, 0.003, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15add1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
