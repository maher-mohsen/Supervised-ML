{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8e-oyIp-SS"
      },
      "source": [
        "## Used libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P0uA9vmfp-SS"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XHx25tncp-ST"
      },
      "source": [
        "# 1. Preprocessing Data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8roVt28_JLTa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0An-OWdmp-ST"
      },
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9BKhFVTnp-ST"
      },
      "source": [
        "## 1.1 Convert data to Data Frame:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XswURE2kp-ST"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame().from_dict(mnist.data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h2aE__Ztp-SU"
      },
      "source": [
        "## 1.2 Filter data with target 0 , 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2xTDyIPfp-SU"
      },
      "outputs": [],
      "source": [
        "df['target'] = mnist.target.astype('int32')\n",
        "df = df.loc[ df['target'].isin( [0 , 1]) ]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5KFuQBN6p-SV"
      },
      "source": [
        "## 1.3 Remove columns that have same value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M9JKYaqJp-SV"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    if (df[col] == 0).all() :\n",
        "        df.drop(col , axis= 1 , inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mYBARmehxLXo"
      },
      "source": [
        "##1.4 Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uAjtdYyO2gFE"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['target'])\n",
        "y = df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oQ7_NW_NxRWL"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-jYTYljpp-SV"
      },
      "source": [
        "## 1.5 standariza Data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sk312JkJp-SV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2NM8YjGYp-SW"
      },
      "source": [
        "# 2. Optimizers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CVpvaOUNp-SW"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self ,eta = 0.01 , epsilon = 1e-7 , beta = 0.99, beta2= 0.99 ):\n",
        "        self.eta = eta\n",
        "        self.epsilon = epsilon\n",
        "        self.beta = beta\n",
        "        self.beta2 = beta2\n",
        "            \n",
        "    def gradient_descent(self , w , b , dw , db) :\n",
        "        w = w - self.eta * dw\n",
        "        b = b - self.eta * db \n",
        "        return w , b      \n",
        "    \n",
        "    def RMS_prop(self , prev_vdw , prev_vdb, dw , db , w , b ):\n",
        "\n",
        "        current_vdw = self.beta * prev_vdw + (1 - self.beta) * ( dw ** 2 ) \n",
        "        current_vdb = self.beta * prev_vdb + (1 - self.beta) * ( db ** 2 )\n",
        "\n",
        "        w = w - self.eta * (dw / ( np.sqrt(current_vdw) + self.epsilon ))\n",
        "        b = b - self.eta * (db / ( np.sqrt(current_vdb) + self.epsilon ))\n",
        "        \n",
        "        return w , b , current_vdw , current_vdb\n",
        "    \n",
        "    def momentum(self , prev_vdw , prev_vdb, dw , db , w , b ):\n",
        "        \n",
        "        current_vdw = self.beta * prev_vdw + (1 - self.beta) * dw \n",
        "        current_vdb = self.beta * prev_vdb + (1 - self.beta) * db \n",
        "        \n",
        "        w = w - self.eta * dw \n",
        "        b = b - self.eta * db \n",
        "        \n",
        "        return w , b , current_vdw , current_vdb\n",
        "    \n",
        "    def Adam(self ,vdw_momentum , vdb_momentum , vdw_rms , vdb_rms , w , b , dw, db, t):\n",
        "        vdw_momentum = self.beta * vdw_momentum + (1 - self.beta) * dw\n",
        "        vdb_momentum = self.beta * vdb_momentum + (1 - self.beta) * db\n",
        "\n",
        "        vdw_rms = self.beta2 * vdw_rms + (1 - self.beta2) * (dw ** 2)\n",
        "        vdb_rms = self.beta2 * vdb_rms + (1 - self.beta2) * (db ** 2)\n",
        "\n",
        "        w = w - self.eta * (vdw_momentum / (np.sqrt(vdw_rms) + self.epsilon))\n",
        "        b = b - self.eta * (vdb_momentum / (np.sqrt(vdb_rms) + self.epsilon))\n",
        "        \n",
        "        return w, b, vdw_momentum, vdb_momentum, vdw_rms, vdb_rms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQTbPpIp-SX"
      },
      "source": [
        "# 3. Logistic Regression Implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ua2yI7e4p-SX"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self , eta = 0.01 , epoches = 100 , tolerance = 1e-7, lambda_ = 5):\n",
        "        self.lambda_ = lambda_\n",
        "        self.eta = eta\n",
        "        self.epoches = epoches\n",
        "        self.tolerance = tolerance\n",
        "    \n",
        "    def sigmoid(self , z):\n",
        "        return 1 / ( 1 + np.exp(-z)) \n",
        "    \n",
        "    \n",
        "    def calculate_dw_logistic(self , X , y ,  y_predicted, w):\n",
        "        length = len(y) \n",
        "        return (1 / length) * ( np.dot(y_predicted - y , X) + ( np.where(w > 0 , self.lambda_ , - self.lambda_ ) ) ) \n",
        "    \n",
        "    def calculate_db_logistic(self , y ,  y_predicted ):\n",
        "        return np.mean( y_predicted - y)\n",
        "    \n",
        "\n",
        "    def yPredict(self ,X , w , b) :\n",
        "        return np.dot(w , X.T) + b\n",
        "    \n",
        "    def identifyClassWithThreshold(self ,y_predict , threshold = 0.5):\n",
        "        for i in range(len(y_predict)):\n",
        "            if y_predict[i] >= threshold :\n",
        "                y_predict[i] = 1\n",
        "            else:\n",
        "                y_predict[i] = 0\n",
        "\n",
        "    \n",
        "    def accuracy(self , y , y_predict):\n",
        "        true = 0 \n",
        "        for (actual , predicted ) in zip(y, y_predict):\n",
        "            if actual == predicted:\n",
        "                true += 1\n",
        "        return ( true / len(y) )   * 100 \n",
        "    \n",
        "    def costWithCrossEntropy(self , y , y_predicted):\n",
        "        length = len(y) \n",
        "        first_term  =  np.dot(-y , np.log(y_predicted) )\n",
        "        second_term =  np.dot((1 - y) , np.log(1 - y_predicted))\n",
        "        result = (1 / length) * (first_term - second_term)\n",
        "        return result \n",
        "    \n",
        "    def fit(self, X , y , batch_size = 1 , algo_optimizer = \"GD\"):\n",
        "\n",
        "        # reset model on re-fit \n",
        "        self.__init__(self.eta,self.epoches,self.tolerance,self.lambda_)\n",
        "\n",
        "        # intializa the weights randomly\n",
        "        w = np.random.rand(X.shape[1]) \n",
        "        b = np.random.rand(1) \n",
        "        length = len(X)\n",
        "        n = len(X[0]) # number of features \n",
        "        \n",
        "        prev_vdw_momentum =  prev_vdb_momentum = prev_vdb_RMS = prev_vdw_RMS = 0\n",
        "        for epoch in range(self.epoches):\n",
        "            last_index = 0 \n",
        "\n",
        "            # batching\n",
        "            for last_index in range(0 , length , batch_size ):\n",
        "\n",
        "                # splitting                 \n",
        "                x_batch = X[last_index : min(last_index + batch_size , length) ]\n",
        "                y_batch = y[last_index : min(last_index + batch_size , length) ]\n",
        "                \n",
        "                \n",
        "                # calculate net value\n",
        "                z = self.yPredict(x_batch , w , b)\n",
        "            \n",
        "                # plugin activation function to net_value to get predicted value\n",
        "                y_predicted = self.sigmoid(z) \n",
        "                \n",
        "                \n",
        "                \n",
        "                # calculating cost with cross entropy\n",
        "                error = self.costWithCrossEntropy(y_batch , y_predicted)\n",
        "                \n",
        "                \n",
        "                if error <= self.tolerance:\n",
        "                    break\n",
        "\n",
        "            \n",
        "                # update the weights \n",
        "                dw =  self.calculate_dw_logistic(x_batch , y_batch , y_predicted, w)\n",
        "                db =  self.calculate_db_logistic( y_batch , y_predicted)\n",
        "                optimizer = Optimizer(eta=self.eta)\n",
        "\n",
        "\n",
        "                # L1 Reg \n",
        "                prev_vdw_RMS = 0\n",
        "                prev_vdb_RMS = 0\n",
        "                prev_vdw_momentum = 0\n",
        "                prev_vdb_momentum = 0\n",
        "                vdw_momentum      = 0\n",
        "                vdb_momentum      = 0\n",
        "                vdw_rms           = 0\n",
        "                vdb_rms           = 0\n",
        "                # optimizers switch\n",
        "                if algo_optimizer == \"GD\":\n",
        "                    w,b = optimizer.gradient_descent(w, b, dw, db)\n",
        "\n",
        "                elif algo_optimizer == \"RMSProp\":\n",
        "                    w, b, prev_vdw_RMS, prev_vdb_RMS = optimizer.momentum(prev_vdw_RMS , prev_vdb_RMS , dw , db ,w , b)\n",
        "\n",
        "                elif algo_optimizer == \"momentum\":\n",
        "                    w, b, prev_vdw_momentum, prev_vdb_momentum = optimizer.RMS_prop(prev_vdw_momentum , prev_vdb_momentum , dw , db ,w , b)\n",
        "\n",
        "                elif algo_optimizer == \"Adam\":\n",
        "                    w, b, vdw_momentum, vdb_momentum, vdw_rms, vdb_rms = optimizer.Adam(vdw_momentum ,vdb_momentum , vdw_rms , vdb_rms ,w , b, dw, db, batch_size)\n",
        "\n",
        "                else:\n",
        "                    print(f\"Optimizer  : {algo_optimizer} optimizer ,  not found !\")\n",
        "                    return np.zeros(n) , 0\n",
        "            # print(f\"Epoch :{epoch} ----------> error :{error}  with learning rate : {self.eta}\")\n",
        "            \n",
        "        return w , b    \n",
        "            \n",
        "        \n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WUAx-rT1-M4z"
      },
      "source": [
        "# 4. Testing and report"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BZq9miLFCaqy"
      },
      "source": [
        "## 4.1 Constants "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nUQFoSpP99zC"
      },
      "outputs": [],
      "source": [
        "optimizers = [ \"Adam\" , \"RMSProp\" , \"momentum\" , \"GD\" ]\n",
        "batches    = [32  , 64 , 128 , 256 , 512]\n",
        "l1_lambdas = [0   , 5  , 10 , 15] \n",
        "etas       = [0.1 , 0.01 , 0.001 , 0.0001 ]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5WdXJuw-CemR"
      },
      "source": [
        "## 4.2 Scaling and preparing test set  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gX2rI-1YBVzK"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_test = scaler.fit_transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvZZtw_CkBi"
      },
      "source": [
        "## 4.3 Try diffrent values form lambda term ( lasso regularization )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFgNIM9tAUcj",
        "outputId": "699c3283-5ec7-4b25-88bc-c41103bfafb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using Gradient descent optimizer , with regularization term = 0 , model accuracy is =  99.61049610496106\n",
            "using Gradient descent optimizer , with regularization term = 5 , model accuracy is =  98.62648626486265\n",
            "using Gradient descent optimizer , with regularization term = 10 , model accuracy is =  96.9249692496925\n",
            "using Gradient descent optimizer , with regularization term = 15 , model accuracy is =  60.311603116031165\n"
          ]
        }
      ],
      "source": [
        "# testing diffrent values for lambda ( Lasso regulariation )\n",
        "for l1_lambda in l1_lambdas : \n",
        "    \n",
        "    clf = LogisticRegression( eta = 0.01 , epoches = 100 , tolerance = 1e-7 , lambda_ = l1_lambda )\n",
        "    \n",
        "    # Mini-batch gradient descent ( of batch size 32 )\n",
        "    w , b = clf.fit( X_train , y_train , 32 , algo_optimizer = \"GD\" )\n",
        "\n",
        "    # prediction \n",
        "    z = clf.yPredict(X_test , w , b)\n",
        "    y_predicted = clf.sigmoid(z) \n",
        "\n",
        "    # classification \n",
        "    clf.identifyClassWithThreshold(y_predicted)\n",
        "    \n",
        "    # calculate accuracy \n",
        "    print(f\"using Gradient descent optimizer , with regularization term = {l1_lambda} , model accuracy is = \" , clf.accuracy(y_test , y_predicted))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bNP5yDIUEUd5"
      },
      "source": [
        "## 4.4  Try diffrent batch sizes ( mini-batchs )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0l_sxR0EhZ6",
        "outputId": "c26a28c8-6103-40bf-b6dc-9aeb020c5990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using Adam optimizer , with batch size 32 , model accuracy is =  96.74046740467405\n",
            "using RMSProp optimizer , with batch size 32 , model accuracy is =  99.73349733497335\n",
            "using momentum optimizer , with batch size 32 , model accuracy is =  96.65846658466585\n",
            "using GD optimizer , with batch size 32 , model accuracy is =  99.50799507995079\n",
            "-------------------------------\n",
            "using Adam optimizer , with batch size 64 , model accuracy is =  97.51947519475195\n",
            "using RMSProp optimizer , with batch size 64 , model accuracy is =  99.65149651496516\n",
            "using momentum optimizer , with batch size 64 , model accuracy is =  97.43747437474374\n",
            "using GD optimizer , with batch size 64 , model accuracy is =  99.58999589995899\n",
            "-------------------------------\n",
            "using Adam optimizer , with batch size 128 , model accuracy is =  98.1959819598196\n",
            "using RMSProp optimizer , with batch size 128 , model accuracy is =  99.73349733497335\n",
            "using momentum optimizer , with batch size 128 , model accuracy is =  98.29848298482985\n",
            "using GD optimizer , with batch size 128 , model accuracy is =  99.73349733497335\n",
            "-------------------------------\n",
            "using Adam optimizer , with batch size 256 , model accuracy is =  98.79048790487906\n",
            "using RMSProp optimizer , with batch size 256 , model accuracy is =  99.63099630996311\n",
            "using momentum optimizer , with batch size 256 , model accuracy is =  98.66748667486675\n",
            "using GD optimizer , with batch size 256 , model accuracy is =  99.54899548995489\n",
            "-------------------------------\n",
            "using Adam optimizer , with batch size 512 , model accuracy is =  99.05699056990571\n",
            "using RMSProp optimizer , with batch size 512 , model accuracy is =  99.36449364493645\n",
            "using momentum optimizer , with batch size 512 , model accuracy is =  98.99548995489955\n",
            "using GD optimizer , with batch size 512 , model accuracy is =  99.2619926199262\n",
            "-------------------------------\n"
          ]
        }
      ],
      "source": [
        "for batch in batches : \n",
        "    for optimizer in optimizers : \n",
        "        # Logistic Regression model \n",
        "        clf = LogisticRegression( eta = 0.01 , epoches = 100 , tolerance = 1e-7 , lambda_ = 0 )\n",
        "    \n",
        "        # diffrent Mini-batchs with diffrent optimizers  \n",
        "        w , b = clf.fit( X_train , y_train , batch , algo_optimizer = optimizer )\n",
        "\n",
        "        # prediction \n",
        "        z = clf.yPredict(X_test , w , b)\n",
        "        y_predicted = clf.sigmoid(z) \n",
        "\n",
        "        # classification \n",
        "        clf.identifyClassWithThreshold(y_predicted)\n",
        "        \n",
        "        # calculate accuracy \n",
        "        print(f\"using {optimizer} optimizer , with batch size {batch} , model accuracy is = \" , clf.accuracy(y_test , y_predicted))\n",
        "    print(\"-------------------------------\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c8nN7uLG-SXq"
      },
      "source": [
        "# 5. Final Conclusion "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HJk3tXEDSvy_"
      },
      "source": [
        "1. Based on the results, it can be concluded that different optimizers and batch sizes can have a significant impact on the model's accuracy. RMSProp optimizer consistently achieved the highest accuracy across all batch sizes, while Adam optimizer showed the second-highest accuracy. On the other hand, GD optimizer consistently performed the worst. Increasing batch size generally resulted in better accuracy, except in the case of momentum optimizer where the accuracy decreased as batch size increased. In summary, choosing the right optimizer and batch size is crucial to achieving high accuracy in machine learning models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kEfunElaXOfP"
      },
      "source": [
        "2. for our problem based on this results and conclusion we may use the RMSProp \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XHx25tncp-ST",
        "2NM8YjGYp-SW",
        "qxQTbPpIp-SX",
        "WUAx-rT1-M4z",
        "c8nN7uLG-SXq"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
