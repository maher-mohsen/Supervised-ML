# Optimizers
Optimization algorithms are essential for finding the optimal set of parameters that minimize the loss function of a machine learning model. This repository includes implementations of the following optimization algorithms:

  -Gradient Descent: A widely used optimization algorithm that iteratively updates model parameters in the direction of steepest descent of the loss function.

  -Stochastic Gradient Descent (SGD): A variant of Gradient Descent that performs parameter updates using a single randomly selected training sample at each iteration. This algorithm is suitable for large-scale datasets.

  -Mini-Batch Gradient Descent: A compromise between Gradient Descent and SGD, where parameter updates are performed using a mini-batch of randomly selected training samples.

  -Adam: An adaptive optimization algorithm that dynamically adjusts the learning rate for each parameter based on past gradients. Adam combines the benefits of AdaGrad and RMSprop to provide efficient and effective      optimization.

![Optimizers](optimizers.gif)
