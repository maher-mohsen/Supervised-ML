{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d84d263",
   "metadata": {},
   "source": [
    "## 1. Load MINST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da700e3e",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9778a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6f910",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0f21703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist.data, mnist.target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7fff33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.0\n",
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2c3b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y.values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d2081f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0076b19",
   "metadata": {},
   "source": [
    "#### K-Fold cross validation implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d06ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549be525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    true=0\n",
    "    for i in range(len(y)) :\n",
    "        if y[i] == y_hat[i]:\n",
    "            true+=1\n",
    "    return (true / len(y)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe3aa9",
   "metadata": {},
   "source": [
    "## 5. Implement Logistic Regression with different values for learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b311c",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d6fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34e56c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, no_classes):\n",
    "    y_new = np.zeros((len(y), no_classes))\n",
    "    for i in range(no_classes):\n",
    "        for j in range(len(y)):\n",
    "            if(y[i] == i):\n",
    "                y_new[j][i] = 1\n",
    "                continue\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2cc48618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72eb451",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0dd18ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac74217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic:\n",
    "    def __init__(self, eta=0.0008, epochs=100, no_classes=10):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.eta = None\n",
    "        self.epochs = None\n",
    "        self.no_classes = None\n",
    "    def unit_step(self, z):\n",
    "        for i in range(len(z)):\n",
    "            if z[i] >0.5:\n",
    "                z[i] = 1\n",
    "            else:\n",
    "                z[i]=0\n",
    "        return z\n",
    "    \n",
    "    def cost(self,y_hat,y):\n",
    "        m = len(y)\n",
    "        return -np.mean(-y * np.log(y_hat) - ((1-y) * np.log(1-y_hat)))\n",
    "    \n",
    "    def get_net_input(self, X, y):\n",
    "        return (np.dot(X, self.w)) + self.b\n",
    "    \n",
    "    def update_weight(self, y_pred, y, X):\n",
    "        return self.w - ((self.eta * np.dot((y_pred - y),X)) / len(X))\n",
    "    \n",
    "    def update_bias(self, y_pred, y):\n",
    "        return self.b - (self.eta * np.mean((y_pred - y)))\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y, eta, epochs, no_classes):\n",
    "        m = len(X)\n",
    "        self.w = np.zeros((10, X.shape[0]))\n",
    "        self.b = np.random.uniform(low=-0.01, high=0.01, size=1)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.no_classes = no_classes\n",
    "        z = np.zeros(no_classes)\n",
    "        for epoch in range(epochs):\n",
    "            for classifer in range(no_classes):\n",
    "            \n",
    "                #Get net input\n",
    "                z = self.get_net_input(X, y)\n",
    "                \n",
    "                #apply activation function\n",
    "                z = self.activation(z)\n",
    "                \n",
    "                #Calc error\n",
    "                y_pred = softmax(z.copy(), self.w[classifer], self.b)\n",
    "                error = self.cost(y_pred, y)\n",
    "                \n",
    "                #Update Weight and bias\n",
    "                self.w[classifer] = self.update_weight(y_pred, y, X)\n",
    "                self.b = self.update_bias(y_pred, y)\n",
    "\n",
    "                if error == 0:\n",
    "                    break\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.unit_step((np.dot(self.w, X.T)) + self.b)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [self.w, self.b]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3e4f1",
   "metadata": {},
   "source": [
    "## 6. Report difference accuracy for the different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "73dfa9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns = ['Target'])\n",
    "y = dataset['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5121ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "test = one_hot_encode(y,10)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b05437d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Logistic(0.008, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00a4ea15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1797,64) and (10,1797) not aligned: 64 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.008\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[107], line 45\u001b[0m, in \u001b[0;36mLogistic.fit\u001b[1;34m(self, X, y, eta, epochs, no_classes)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m classifer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_classes):\n\u001b[0;32m     43\u001b[0m     \n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m#Get net input\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_net_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;66;03m#apply activation function\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n",
      "Cell \u001b[1;32mIn[107], line 21\u001b[0m, in \u001b[0;36mLogistic.get_net_input\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_net_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1797,64) and (10,1797) not aligned: 64 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "clf.fit(X, y, 0.008, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
